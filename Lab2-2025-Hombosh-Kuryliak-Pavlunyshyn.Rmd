---
title: 'P&S-2022: Lab assignment 2'
author: "Куриляк Данило, Павлунишин Богдан, Гомбош Олег"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with
    explanations + oral defense). Submission deadline **November 1,
    2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to
    **cms** both the source *R notebook* **and** the generated html
    file\
-   At the beginning of the notebook, provide a work-breakdown structure
    estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer
        to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is
        just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you
        use to complete the task) as well as histograms etc to
        illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding
        theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree
        with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit**
    ordinal number of your team on the list. Include the line
    **set.seed(team id number)** at the beginning of your code to make
    your calculations reproducible. Also observe that the answers **do**
    depend on this number!\
-   Take into account that not complying with these instructions may
    result in point deduction regardless of whether or not your
    implementation is correct.

------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it
    to a $7$-bit *codeword*
    $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where
    $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the
    received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome
    vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$
    *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary
    $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no.
    $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or
    more than one), while $(1 1 0 )$ means the third bit (or more than
    one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in
    $\mathbf{r}$ to get the corrected
    $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### We assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = 0.02$, where $\mathtt{id}$ is the number of out team.

1.  Simulate the encoding-transmission-decoding process $N$ times and
    find the estimate $\hat p$ of the probability $p^*$ of correct
    transmission of a single message $\mathbf{m}$. Comment why, for
    large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator
    of success by the standard error of your sample and using the CLT,
    predict the \emph{confidence} interval
    $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate
    $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while
    transmitting a $4$-digit binary message. Do you think it is one of
    the known distributions?

#### 

#### Parameters and matrices

```{r}

id <- 2

set.seed(id)
p <- id/100

G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
```

```{r}
cat("The matrix G is: \n") 
G  
cat("The matrix H is: \n") 
H
cat("The product GH must be zero: \n")
(G%*%H) %%2
```

#### Message generation and transmission

```{r}
N <- 100000

message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
messages <- message_generator(100000)
codewords <- (messages %*% G) %% 2

errors <- matrix(rbinom(N*7, 1, 0.02), nrow=N)
received <- (codewords + errors) %% 2;
```

```{r}
cat("Messages generated are : \n")
head(messages)
cat("\n")
cat("Encoded messages are: \n")
head(codewords)
cat("\n")
cat("Received messages are: \n")
head(received)
cat("\n")
cat("Mean of errors is : ")
mean(errors)
```

#### **Error detection and correction**

The 3-bit syndrome is a binary index of the erroneous bit.

```{r}
syndrome <- (received %*% H) %% 2
syndrome_index <- syndrome[,1] + 2*syndrome[,2] + 4*syndrome[,3]

corrected <- received
for (i in 1:N) {
  if (syndrome_index[i] != 0)
    corrected[i, syndrome_index[i]] <- (corrected[i, syndrome_index[i]] + 1) %% 2
}
```

```{r}
cat("Failed bits are: \n")
table(syndrome_index)
```

#### Decoding and probability estimate

```{r}
decoded <- corrected[, c(3,5,6,7)]
success <- rowSums(decoded == messages) == 4
p_hat   <- mean(success)
```

```{r}
cat("Proportion of correctly decoded messages: ")
p_hat
```

#### Confidence interval and sample-size condition

```{r}
se  <- sqrt(p_hat*(1 - p_hat)/N)
eps <- 1.96 * se
ci  <- c(p_hat - eps, p_hat + eps)
N_required <- p_hat*(1 - p_hat)*(1.96/0.03)^2
```

```{r}
cat("Required N for precision ε = 0.03: ")
N_required
```

### Histogram of bit errors

```{r}
bit_errors <- rowSums(errors)
hist(bit_errors, breaks=0:7,
     main="Histogram of Bit Errors per Transmission",
     xlab="Number of flipped bits (k)", ylab="Frequency")
mean(bit_errors); var(bit_errors)
```

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a
Poisson distribution. As you remember, a Poisson random variable
describes occurrences of rare events, i.e., counts the number of
successes in a large number of independent random experiments. One of
the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big
*half-life period* $T$; it is vitally important to know the probability
that during a one second period, the number of nuclei decays will not
exceed some critical level $k$. This probability can easily be estimated
using the fact that, given the *activity* ${\lambda}$ of the element
(i.e., the probability that exactly one nucleus decays in one second)
and the number $N$ of atoms in the sample, the random number of decays
within a second is well modelled by Poisson distribution with parameter
$\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms
is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro
constant, and $M$ is the molar (atomic) mass of the element. The
activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is
measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive
element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life
period $T = 30.1$ years and mass
$m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by
$X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays
in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll
    need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$
    gets very close to a normal one as $n$ becomes large and identify
    that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and
        calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical
        cumulative distribution function $\hat F_{\mathbf{s}}$ of
        $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$
        of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.}
        $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to
        visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two
        \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the
        results.\
3.  Calculate the largest possible value of $n$, for which the total
    number of decays in one second is less than $8 \times 10^8$ with
    probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality,
        Chernoff bound and Central Limit Theorem, and compare the
        results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and
        calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less
        than critical value ($8 \times 10^8$) and calculate the
        empirical probability; comment whether it is close to the
        desired level $0.95$

```{r}
lambda <- 136.907
N <- 100
mu <- N * lambda
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
```

# PART 1: Calculate the Poisson parameter

```{r}
id <- 2
T <- 30.1

m <- id * 1e-6
M <- 136.907
Na <- 6e23

T_seconds <- T * 365.25 * 24 * 3600

atoms_number <- m / M * Na
lambda <- log(2) / T_seconds

mu <- atoms_number * lambda

mu

```

# PART 2: Central Limit Theorem Analysis

```{r}
K <- 10000

analyze_sample_means <- function(n, mu, K) {
  sample_means <- replicate(K, mean(rpois(n, lambda = mu)))
  
  theoretical_mean <- mu
  theoretical_sd <- sqrt(mu / n)
  
  Fs <- ecdf(sample_means)
  
  x_vals <- seq(min(sample_means), max(sample_means), length.out = 1000)
  empirical_cdf <- Fs(x_vals)
  theoretical_cdf <- pnorm(x_vals, mean = theoretical_mean, sd = theoretical_sd)
  max_diff <- max(abs(empirical_cdf - theoretical_cdf))

  
  xlims <- c(theoretical_mean - 4*theoretical_sd, 
             theoretical_mean + 4*theoretical_sd)
  
  plot(Fs, 
       xlim = xlims, 
       ylim = c(0, 1),
       col = "blue",
       lwd = 2,
       main = paste("ECDF vs Normal CDF for Sample Means (n =", n, ")"),
       xlab = "Sample Mean",
       ylab = "Cumulative Probability")
  
  curve(pnorm(x, mean = theoretical_mean, sd = theoretical_sd), 
        col = "red", 
        lwd = 2, 
        add = TRUE)
  
  legend("topleft", 
         legend = c("Empirical CDF", 
                   paste0("Normal(μ=", sprintf("%.2e", theoretical_mean), 
                         ", σ=", sprintf("%.2e", theoretical_sd), ")"),
                   paste("Max difference =", sprintf("%.6f", max_diff))),
         col = c("blue", "red", "black"),
         lwd = c(2, 2, 0),
         bty = "n")
  
  return(list(mean = theoretical_mean, 
              sd = theoretical_sd, 
              max_diff = max_diff,
              sample_means = sample_means))
}

```

```{r}
par(mfrow = c(1, 3))
results_n5 <- analyze_sample_means(5, mu, K)
results_n10 <- analyze_sample_means(10, mu, K)
results_n50 <- analyze_sample_means(50, mu, K)
par(mfrow = c(1, 1))

cat("\nResults for n = 5:\n")
cat("  Theoretical mean:", sprintf("%.3e", results_n5$mean), "\n")
cat("  Theoretical SD:", sprintf("%.3e", results_n5$sd), "\n")
cat("  Max difference:", sprintf("%.6f", results_n5$max_diff), "\n")

cat("\nResults for n = 10:\n")
cat("  Theoretical mean:", sprintf("%.3e", results_n10$mean), "\n")
cat("  Theoretical SD:", sprintf("%.3e", results_n10$sd), "\n")
cat("  Max difference:", sprintf("%.6f", results_n10$max_diff), "\n")

cat("\nResults for n = 50:\n")
cat("  Theoretical mean:", sprintf("%.3e", results_n50$mean), "\n")
cat("  Theoretical SD:", sprintf("%.3e", results_n50$sd), "\n")
cat("  Max difference:", sprintf("%.6f", results_n50$max_diff), "\n")

cat("\nComment: As n increases, the maximum difference decreases,")
cat("\nshowing that the CLT approximation improves with larger n.\n\n")
```

# PART 3: Maximum n for Safety Constraint

```{r}

critical_value <- 8e8
target_probability <- 0.95

estimate_probability <- function(n, mu, critical_value, K = 10000) {
  sums <- replicate(K, sum(rpois(n, lambda = mu)))
  empirical_prob <- mean(sums < critical_value)
  return(list(prob = empirical_prob, sums = sums))
}

calculate_bounds <- function(n, mu, critical_value) {
  total_mean <- n * mu
  total_var <- n * mu
  total_sd <- sqrt(total_var)
  
  if (total_mean < critical_value) {
    markov_bound <- 1 - total_mean / critical_value
  } else {
    markov_bound <- 0
  }

  if (total_mean < critical_value) {
    t <- critical_value / total_mean
    chernoff_bound <- 1 - exp(-total_mean * (t * log(t) - t + 1))
  } else {
    chernoff_bound <- 0
  }
  
  clt_prob <- pnorm(critical_value, mean = total_mean, sd = total_sd)
  
  return(list(markov = markov_bound, 
              chernoff = chernoff_bound, 
              clt = clt_prob,
              mean = total_mean,
              sd = total_sd))
}

```

# Searching for perfect n

```{r}
n_min <- 1
n_max <- 10000

while (n_max - n_min > 1) {
  n_test <- floor((n_min + n_max) / 2)
  result <- estimate_probability(n_test, mu, critical_value, K = 5000)
  
  if (result$prob >= target_probability) {
    n_min <- n_test
  } else {
    n_max <- n_test
  }
  
  cat("Testing n =", n_test, ", P(S < critical) =", 
      sprintf("%.4f", result$prob), "\n")
}

n_final <- n_min
cat("\nFinal test with K = 10000...\n")
final_result <- estimate_probability(n_final, mu, critical_value, K = 10000)
bounds <- calculate_bounds(n_final, mu, critical_value)

cat("\n=== RESULTS ===\n")
cat("Maximum n:", n_final, "\n")
cat("Empirical probability:", sprintf("%.4f", final_result$prob), "\n\n")

cat("Theoretical bounds:\n")
cat("  Markov inequality: P(S < critical) ≥", sprintf("%.4f", bounds$markov), "\n")
cat("  Chernoff bound:    P(S < critical) ≥", sprintf("%.4f", bounds$chernoff), "\n")
cat("  CLT approximation: P(S < critical) ≈", sprintf("%.4f", bounds$clt), "\n\n")

cat("Parameters for S = X₁ + ... + Xₙ:\n")
cat("  E[S] = n*μ =", sprintf("%.3e", bounds$mean), "\n")
cat("  SD[S] = √(n*μ) =", sprintf("%.3e", bounds$sd), "\n\n")
```

# Visualization for better understanding

```{r}
hist(final_result$sums, 
     breaks = 50,
     col = "lightblue",
     border = "white",
     main = paste("Distribution of Total Decays (n =", n_final, ")"),
     xlab = "Total Number of Decays",
     freq = FALSE,
     xlim = c(min(final_result$sums), max(final_result$sums, critical_value)))

# Add normal density curve
curve(dnorm(x, mean = bounds$mean, sd = bounds$sd), 
      col = "red", lwd = 2, add = TRUE)

abline(v = critical_value, col = "darkgreen", lwd = 2, lty = 2)
abline(v = bounds$mean, col = "orange", lwd = 2, lty = 3)

legend("topright", 
       legend = c("Empirical distribution",
                 "Normal approximation",
                 "Critical value",
                 "Mean",
                 paste("P(S < critical) =", sprintf("%.4f", final_result$prob))),
       col = c("lightblue", "red", "darkgreen", "orange", "black"),
       lwd = c(10, 2, 2, 2, 0),
       lty = c(1, 1, 2, 3, 0),
       bty = "n")

# Visualization 2: ECDF vs Theoretical CDF
xlims <- c(bounds$mean - 4*bounds$sd, bounds$mean + 4*bounds$sd)
Fs <- ecdf(final_result$sums)

plot(Fs, 
     xlim = xlims, 
     ylim = c(0, 1),
     col = "blue",
     lwd = 2,
     main = paste("ECDF vs Normal CDF for Total Decays (n =", n_final, ")"))

curve(pnorm(x, mean = bounds$mean, sd = bounds$sd), 
      col = "red", lwd = 2, add = TRUE)

abline(v = critical_value, col = "darkgreen", lwd = 2, lty = 2)
abline(h = target_probability, col = "purple", lwd = 1, lty = 3)

legend("topleft",
       legend = c("Empirical CDF", 
                 "Normal CDF",
                 "Critical value",
                 "Target probability"),
       col = c("blue", "red", "darkgreen", "purple"),
       lwd = c(2, 2, 2, 1),
       lty = c(1, 1, 2, 3),
       bty = "n")
```

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of
    $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as
    $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the
        \textbf{r.v.} $X_i$ and calculate the sample mean
        $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the
        \emph{empirical cumulative distribution} function
        $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of
        $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.}
        $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph
        to visualize their proximity;\
    -   calculate the maximal difference between the two
        \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the
        results.
2.  The place can be considered safe when the number of clicks in one
    minute does not exceed $100$. It is known that the parameter $\nu$
    of the resulting exponential distribution is proportional to the
    number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where
    $\nu_1$ is the parameter for one sample. Determine the maximal
    number of radioactive samples that can be stored in that place so
    that, with probability $0.95$, the place is identified as safe. To
    do this,
    -   express the event of interest in terms of the \textbf{r.v.}
        $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov
        inequality, Chernoff bound and Central Limit Theorem and compare
        the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization
        $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum
        $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the
        $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe
        and compare to the desired level $0.95$

#### First, generate samples an sample means:

```{r}
team_id <- 2
nu1 <- team_id + 10
K <- 10000

analyze_exponential_means <- function(n, nu1, K) {
  sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow = n))

  mu <- 1 / nu1
  sigma <- 1 / (nu1 * sqrt(n))
  
  Fs <- ecdf(sample_means)
  
  x_vals <- seq(min(sample_means), max(sample_means), length.out = 1000)
  empirical_cdf <- Fs(x_vals)
  theoretical_cdf <- pnorm(x_vals, mean = mu, sd = sigma)
  max_diff <- max(abs(empirical_cdf - theoretical_cdf))
  
  xlims <- c(mu - 4*sigma, mu + 4*sigma)
  
  plot(Fs, 
       xlim = xlims, 
       ylim = c(0, 1),
       col = "blue",
       lwd = 2,
       main = paste("ECDF vs Normal CDF for Sample Means (n =", n, ")"),
       xlab = "Sample Mean (time between clicks)",
       ylab = "Cumulative Probability")
  
  curve(pnorm(x, mean = mu, sd = sigma), 
        col = "red", 
        lwd = 2, 
        add = TRUE)
  
  legend("topleft", 
         legend = c("Empirical CDF", 
                   paste0("Normal(μ=", sprintf("%.4f", mu), 
                         ", σ=", sprintf("%.4f", sigma), ")"),
                   paste("Max difference =", sprintf("%.6f", max_diff))),
         col = c("blue", "red", "black"),
         lwd = c(2, 2, 0),
         bty = "n")
  
  return(list(mu = mu, 
              sigma = sigma, 
              max_diff = max_diff,
              sample_means = sample_means))
}

par(mfrow = c(1, 3))
results_n5 <- analyze_exponential_means(5, nu1, K)
results_n10 <- analyze_exponential_means(10, nu1, K)
results_n50 <- analyze_exponential_means(50, nu1, K)
par(mfrow = c(1, 1))
```

```{r}
# Analyze for n = 5, 10, 50
par(mfrow = c(1, 3))
results_n5 <- analyze_exponential_means(5, nu1, K)
results_n10 <- analyze_exponential_means(10, nu1, K)
results_n50 <- analyze_exponential_means(50, nu1, K)
par(mfrow = c(1, 1))

cat("\nResults for n = 5:\n")
cat("  Theoretical μ:", sprintf("%.6f", results_n5$mu), "\n")
cat("  Theoretical σ:", sprintf("%.6f", results_n5$sigma), "\n")
cat("  Max difference:", sprintf("%.6f", results_n5$max_diff), "\n")

cat("\nResults for n = 10:\n")
cat("  Theoretical μ:", sprintf("%.6f", results_n10$mu), "\n")
cat("  Theoretical σ:", sprintf("%.6f", results_n10$sigma), "\n")
cat("  Max difference:", sprintf("%.6f", results_n10$max_diff), "\n")

cat("\nResults for n = 50:\n")
cat("  Theoretical μ:", sprintf("%.6f", results_n50$mu), "\n")
cat("  Theoretical σ:", sprintf("%.6f", results_n50$sigma), "\n")
cat("  Max difference:", sprintf("%.6f", results_n50$max_diff), "\n")
```

As n increases from 5 to 50, the maximum difference decreases,
demonstrating that the normal approximation improves with larger sample
sizes. The Central Limit Theorem holds well even for moderately small n.

### Let Xᵢ \~ Exp(ν) be time between clicks (i-1) and i

### S = X₁ + ... + X₁₀₀ is total time for 100 clicks

### Safe if: S \> 60 seconds (i.e., 100 clicks take more than 60 sec)

### Equivalently: P(S \> 60) ≥ 0.95, or P(S ≤ 60) ≤ 0.05

```{r}
calculate_max_N_bounds <- function(nu1, n_clicks = 100, time_limit = 60, target_prob = 0.05) {
  results <- list()

  markov_N <- n_clicks / (nu1 * time_limit / target_prob)
  results$markov <- floor(markov_N)

  chernoff_search <- function(N) {
    nu <- N * nu1
    lambda_param <- n_clicks / time_limit
    if (nu <= lambda_param) return(1)  # bound is useless
    t_opt <- nu - lambda_param
    bound <- ((nu / (nu + t_opt)) ^ n_clicks) * exp(t_opt * time_limit)
    return(bound)
  }
  
  N_test <- seq(1, 100, by = 0.1)
  chernoff_bounds <- sapply(N_test, chernoff_search)
  N_chernoff <- max(N_test[chernoff_bounds <= target_prob])
  results$chernoff <- floor(N_chernoff)
  
  z_alpha <- qnorm(target_prob)

  N_clt <- (n_clicks + 10 * z_alpha) / (time_limit * nu1)
  results$clt <- floor(N_clt)
  
  return(results)
}
```

# MARKOV INEQUALITY: P(S ≤ a) cannot be directly bounded by Markov

\# But we can use: P(S \> a) ≤ E[S]/a only if E[S] ≥ a \# We need P(S \>
60) ≥ 0.95, so E[S] should be large \# Markov gives: P(S ≤ 60) ≤ 1 -
E[S]/60 (not useful when E[S] \< 60) \# Better approach: E[S] = 60/0.05
= 1200 to ensure the tail is small enough \# This is very conservative

# CHERNOFF BOUND for exponential sum

\# For S \~ Gamma(n, ν), the MGF is M_S(t) = (ν/(ν-t))\^n for t \< ν \#
P(S ≤ a) = P(e\^(-tS) ≥ e\^(-ta)) ≤ E[e\^(-tS)]/e\^(-ta) = (ν/(ν+t))\^n
\* e\^(ta) \# Optimize over t \> 0 \# Optimal t = ν - n/a when n/a \< ν
\# This gives Chernoff bound: P(S ≤ a) ≤ (a*ν/n* e\^(1 - a\*ν/n))\^n

# CENTRAL LIMIT THEOREM

\# S ≈ N(n/ν, n/ν²) = N(100/(N*ν₁), 100/(N*ν₁)²) \# P(S ≤ 60) = Φ((60 -
100/(N*ν₁)) / (10/(N*ν₁))) \# We want Φ((60 - 100/(N*ν₁)) / (10/(N*ν₁)))
≤ 0.05 \# So (60 - 100/(N*ν₁)) / (10/(N*ν₁)) ≤ Φ\^(-1)(0.05) ≈ -1.645 \#
(60*N*ν₁ - 100) / 10 ≤ -1.645 \# 60*N*ν₁ - 100 ≤ -16.45 \# N ≤ 83.55 /
(60\*ν₁)

# More careful: (60 - μ)/σ = z_α where μ = 100/(N*ν₁), σ = 10/(N*ν₁)

\# 60 - 100/(N*ν₁) = z_α* 10/(N*ν₁) \# 60*N*ν₁ - 100 = 10*z_α \# N =
(100 + 10*z_α) / (60*ν₁)

```{r}
bounds <- calculate_max_N_bounds(nu1)

cat("  Markov inequality: N ≤", bounds$markov, "(very conservative)\n")
cat("  Chernoff bound:    N ≤", bounds$chernoff, "\n")
cat("  CLT approximation: N ≤", bounds$clt, "\n\n")

N_predicted <- bounds$clt

if (is.na(N_predicted) || N_predicted <= 0) {
  cat("Warning: CLT prediction is invalid. Using conservative estimate.\n")
  N_predicted <- max(1, floor(bounds$chernoff))
}

if (N_predicted < 1) N_predicted <- 1

nu_predicted <- N_predicted * nu1

cat("Using N =", N_predicted, ", ν =", nu_predicted, "\n\n")
cat("(c) & (d) Simulation with N =", N_predicted, "\n")

simulate_safety <- function(N, nu1, n_clicks = 100, time_limit = 60, K = 10000) {
  if (N <= 0 || is.na(N)) {
    return(list(total_times = rep(NA, K), 
                prob_safe = NA,
                nu = NA))
  }
  
  nu <- N * nu1
  
  total_times <- replicate(K, sum(rexp(n_clicks, rate = nu)))
  prob_safe <- mean(total_times > time_limit)
  
  return(list(total_times = total_times, 
              prob_safe = prob_safe,
              nu = nu))
}

simulation_result <- simulate_safety(N_predicted, nu1, K = K)

if (!is.na(simulation_result$prob_safe) && simulation_result$prob_safe >= 0.95) {
  cat("✓ The location is safe with the predicted number of samples.\n")
} else {
  cat("✗ The predicted N is too high. Need to reduce N.\n")
  N_min <- 1
  N_max <- N_predicted
  
  cat("\nRefining N through binary search...\n")
  while (N_max - N_min > 1) {
    N_test <- floor((N_min + N_max) / 2)
    test_result <- simulate_safety(N_test, nu1, K = 5000)
    
    if (test_result$prob_safe >= 0.95) {
      N_min <- N_test
    } else {
      N_max <- N_test
    }
    cat("Testing N =", N_test, ", P(safe) =", sprintf("%.4f", test_result$prob_safe), "\n")
  }
  
  N_predicted <- N_min
  nu_predicted <- N_predicted * nu1
  simulation_result <- simulate_safety(N_predicted, nu1, K = K)
  
  cat("\nFinal N =", N_predicted, "\n")
  cat("Final P(safe) =", sprintf("%.4f", simulation_result$prob_safe), "\n")
}

```



#### Additional visualization

```{r}

par(mfrow = c(1, 2))

hist(simulation_result$total_times, 
     breaks = 50,
     col = "lightblue",
     border = "white",
     main = paste("Distribution of Total Time (N =", N_predicted, ")"),
     xlab = "Total Time for 100 clicks (seconds)",
     freq = FALSE)

mean_S <- 100 / nu_predicted
sd_S <- 10 / nu_predicted
curve(dnorm(x, mean = mean_S, sd = sd_S), 
      col = "red", lwd = 2, add = TRUE)

abline(v = 60, col = "darkgreen", lwd = 2, lty = 2)
abline(v = mean_S, col = "orange", lwd = 2, lty = 3)

legend("topright", 
       legend = c("Empirical", "Normal approx.", "Time limit (60s)", "Mean"),
       col = c("lightblue", "red", "darkgreen", "orange"),
       lwd = c(10, 2, 2, 2),
       lty = c(1, 1, 2, 3),
       bty = "n")

```


#### We can now plot ecdf and cdf

```{r}
Fs <- ecdf(simulation_result$total_times)
xlims <- c(mean_S - 4*sd_S, mean_S + 4*sd_S)

plot(Fs, 
     xlim = xlims,
     ylim = c(0, 1),
     col = "blue",
     lwd = 2,
     main = "ECDF vs Normal CDF",
     xlab = "Total Time (seconds)",
     ylab = "Cumulative Probability")

curve(pnorm(x, mean = mean_S, sd = sd_S), 
      col = "red", lwd = 2, add = TRUE)

abline(v = 60, col = "darkgreen", lwd = 2, lty = 2)
abline(h = 0.05, col = "purple", lwd = 1, lty = 3)

legend("topleft",
       legend = c("Empirical CDF", "Normal CDF", "Time limit", "α = 0.05"),
       col = c("blue", "red", "darkgreen", "purple"),
       lwd = c(2, 2, 2, 1),
       lty = c(1, 1, 2, 3),
       bty = "n")

par(mfrow = c(1, 1))
```


------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:**

1.  **In this part, we discuss independence of random variables and its
    moments: expectation and variance.** å
2.  Suppose we have a random variable $X$. Explain why
    $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

Explanation: E(1/X) != 1/E(X) because expectation is not linear for
nonlinear transformations.

2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and
    $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations
    $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of
    $Y := \frac{1}{X}$ to calculate the values of
    $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$.
    Comment on the received results;

```{r}
mu <- id
sigma_sq <- 2 * id + 7
sigma <- sqrt(sigma_sq)
N <- 100

set.seed(id)
x_sim <- rnorm(N, mean = mu, sd = sigma)
y_sim <- 1 / x_sim

reciprocal_of_mean <- 1 / mean(x_sim)
mean_of_reciprocal <- mean(y_sim)

print(paste("Parameters: mu =", mu, ", sigma^2 =", sigma_sq))
print(paste("1 / mean(X):", round(reciprocal_of_mean, 6)))
print(paste("mean(1/X):", round(mean_of_reciprocal, 6)))
```

3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter
    $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile
    plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot
    and scatterplot of $X$ and $Z$. Explain the results. Comment on the
    difference of relations between the pairs of random variables. Which
    pair of r.v.'s is dependent and which one is similar?

```{r}
N_plots <- 1000
lambda <- 2

set.seed(id)
x <- rexp(N_plots, rate = lambda)
y <- rexp(N_plots, rate = lambda)
z <- log(x) + 5

par(mfrow = c(2, 2))

# Independent variables X and Y (same distribution)
plot(x, y, main = "Scatterplot: X vs Y (Independent)",
     xlab = "X", ylab = "Y", pch = ".", col = "blue")
qqplot(x, y, main = "Q-Q Plot: X vs Y (Similar distributions)",
       xlab = "Quantiles of X", ylab = "Quantiles of Y")
abline(0, 1, col = "red", lwd = 2)

# Dependent variables X and Z
plot(x, z, main = "Scatterplot: X vs Z (Dependent)",
     xlab = "X", ylab = "Z", pch = ".", col = "darkgreen")
qqplot(x, z, main = "Q-Q Plot: X vs Z (Different distributions)",
       xlab = "Quantiles of X", ylab = "Quantiles of Z")
abline(0, 1, col = "red", lwd = 2)

par(mfrow = c(1, 1))


# Comment: X and Y are independent but similarly distributed.
# X and Z are dependent; transformation creates correlation.
```

```         
------------------------------------------------------------------------
```

2.  You toss a fair coin three times and a random variable $X$ records
    how many times the coin shows Heads. You convince your friend that
    they should play a game with the following payoff: every round
    (equivalent to three coin tosses) will cost £$1$. They will receive
    £$0.5$ for every coin showing Heads. What is the expected value and
    the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X:

        -   Normally distributed

        -   Binomially distributed

        -   Poisson distributed

        -   Uniformly distributed

The random variable $X$ represents the total number of "Successes"
(defined as getting Heads) in a fixed number of independent trials
($n=3$ coin tosses). The probability of success in each trial is
constant ($p=0.5$ for a fair coin).This is the classic definition of a
Binomially distributed (Binomially distributed) random
variable.Therefore, $X \sim \text{Bin}(n=3, p=0.5)$.

2.  What are the expected value and variance of X? Simulate realizations
    $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample
    mean $\overline{\mathbf{X}}$ and sample variance
    $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$.
    Comment on the results;

```{r}
N <- 100
n_trials <- 3
p_success <- 0.5

set.seed(id)
x_sim <- rbinom(N, size = n_trials, prob = p_success)

sample_mean_x <- mean(x_sim)
sample_var_x <- var(x_sim)

print(paste("Theoretical E(X):", 1.5))
print(paste("Sample mean X:", sample_mean_x))
print(paste("Theoretical Var(X):", 0.75))
print(paste("Sample variance X:", sample_var_x))
# Comment: sample mean and variance are close to theoretical values.
```

3.  What are the expected value and variance of Y? Simulate realizations
    $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample
    mean $\overline{\mathbf{Y}}$ and sample variance
    $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$.
    Comment on the results;

```{r}
y_sim <- 0.5 * x_sim - 1

sample_mean_y <- mean(y_sim)
sample_var_y <- var(y_sim)

print(paste("Theoretical E(Y):", -0.25))
print(paste("Sample mean Y:", sample_mean_y))
print(paste("Theoretical Var(Y):", 0.1875))
print(paste("Sample variance Y:", sample_var_y))
# Comment: Expected value of Y is negative, meaning the game is not profitable.
```

We empirically confirmed the non-linearity of expectation
($\mathbb{E}(1/X) \neq 1/\mathbb{E}(X)$) 11, used scatterplots and Q-Q
plots to visually differentiate between independent and functionally
dependent random variables 12, and validated the theoretical mean and
variance of the Binomial distribution through simulation.

------------------------------------------------------------------------

### General summary and conclusions

Summarize here what you've done, whether you solved the tasks, what
difficulties you had etc.
